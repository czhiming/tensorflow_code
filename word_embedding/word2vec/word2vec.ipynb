{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoding:utf-8\n",
    "\n",
    "from input_data import *\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/text8'\n",
    "batch_size = 1200\n",
    "win_size = 3\n",
    "word_dim = 100\n",
    "neg_size = 64\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word num:  31893\n",
      "store vocab is over\n",
      "vocab num: 31893\n"
     ]
    }
   ],
   "source": [
    "#准备训练数据\n",
    "data_loader = TextLoader(data_dir, batch_size, win_size, mini_frq=20)\n",
    "vocab_size = data_loader.vocab_size\n",
    "print 'vocab num:', vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#准备测试例子\n",
    "test_words = ['China', 'good', 'new', 'one'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#模型定义\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #输入变量\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "\n",
    "    #模型参数\n",
    "    with tf.variable_scope('word2vec' + 'embedding'):\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocab_size, word_dim],\n",
    "                                                   -1.0, 1.0))\n",
    "        embeddings = tf.nn.l2_normalize(embeddings, 1)\n",
    "\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([vocab_size, word_dim],\n",
    "                                                      stddev=1.0 / math.sqrt(word_dim)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    labels = tf.expand_dims(train_labels, 1)\n",
    "\n",
    "    # loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights, nce_biases, embed,\n",
    "    #                                      labels, neg_size, vocab_size))\n",
    "\n",
    "    if labels.dtype != tf.int64:\n",
    "            labels = tf.cast(labels, tf.int64)\n",
    "    labels_flat = tf.reshape(labels, [-1])\n",
    "\n",
    "    #第一部分,抽取负例,计算正负例得分\n",
    "    sampled, true_expected_count, sampled_expected_count = tf.nn.log_uniform_candidate_sampler(\n",
    "      true_classes=labels,\n",
    "      num_true=1,\n",
    "      num_sampled=neg_size,\n",
    "      unique=True,\n",
    "      range_max=vocab_size)\n",
    "\n",
    "    all_ids = tf.concat(0, [labels_flat, sampled])\n",
    "\n",
    "    all_w = tf.nn.embedding_lookup(nce_weights, all_ids) #[batch+neg,dim]\n",
    "    all_b = tf.nn.embedding_lookup(nce_biases, all_ids) #[batch+neg]\n",
    "\n",
    "    true_w = tf.slice(all_w, tf.pack([0, 0]), [batch_size, word_dim])\n",
    "    true_b = tf.slice(all_b, [0], [batch_size])\n",
    "    true_logits = tf.matmul(embed, true_w, transpose_b=True) + true_b\n",
    "\n",
    "    sampled_w = tf.slice(all_w, tf.pack([batch_size, 0]), [neg_size, word_dim])\n",
    "    sampled_b = tf.slice(all_b, [batch_size], [neg_size])\n",
    "    sampled_logits = tf.matmul(embed, sampled_w, transpose_b=True) + sampled_b\n",
    "\n",
    "    if True: #减去词出现的先验频率\n",
    "      true_logits -= tf.log(true_expected_count)\n",
    "      sampled_logits -= tf.log(sampled_expected_count)\n",
    "    out_logits = tf.concat(1, [true_logits, sampled_logits])\n",
    "    out_targets = tf.concat(1, [tf.ones_like(true_logits), tf.zeros_like(sampled_logits)])\n",
    "\n",
    "    #第二部分：计算正负例与正确标签的交叉熵交叉熵\n",
    "    #logits,[batch,1+neg_num],[batch,1+neg_num]\n",
    "    #sigmoid_cross_entropy_with_logits(logits, targets)\n",
    "    loss_batchs = tf.nn.relu(out_logits) - out_logits * out_targets \\\n",
    "                  + tf.log(1 + tf.exp(-tf.abs(out_logits)))\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(loss_batchs, 1))\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    #输出词向量\n",
    "    embeddings_norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / embeddings_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss batch: 477.497\n",
      "loss batch: 464.652\n",
      "0/50, train_loss = 0.033, time/batch = 1.463\n",
      "the nearest words with <good> is : good,ju,collateral,rutland,funerals,tangled,auto,stimulate,carolingian,tetragrammaton,schumann\n",
      "the nearest words with <new> is : new,superfamily,portrayal,silent,lacked,pike,siena,immoral,bec,hezekiah,realtime\n",
      "the nearest words with <one> is : one,consolation,lowering,rei,survived,foldoc,aluminium,cranmer,whose,celebration,perceptual\n",
      "loss batch: 455.802\n",
      "loss batch: 418.836\n",
      "1/50, train_loss = 0.031, time/batch = 1.252\n",
      "the nearest words with <good> is : good,ju,collateral,rutland,funerals,tangled,auto,stimulate,carolingian,tetragrammaton,schumann\n",
      "the nearest words with <new> is : new,superfamily,portrayal,silent,lacked,pike,siena,immoral,bec,hezekiah,realtime\n",
      "the nearest words with <one> is : one,consolation,lowering,rei,survived,foldoc,aluminium,whose,celebration,cranmer,perceptual\n"
     ]
    }
   ],
   "source": [
    "#模型训练\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    #for e in range(num_epochs):\n",
    "    for e in range(2):\n",
    "        data_loader.reset_batch_pointer()\n",
    "        start = time.time()\n",
    "        loss_all = 0\n",
    "        #for b in range(data_loader.num_batches):\n",
    "        for b in range(2):\n",
    "            batch_inputs, batch_labels = data_loader.next_batch()\n",
    "            feed = {train_inputs: batch_inputs,\n",
    "                    train_labels: batch_labels}\n",
    "            loss_val,  _ = sess.run([loss, optimizer], feed)\n",
    "            loss_all += loss_val\n",
    "            print 'loss batch:', loss_val\n",
    "        end = time.time()\n",
    "        print(\"{}/{}, train_loss = {:.3f}, time/batch = {:.3f}\" .format(\n",
    "                    e, num_epochs, loss_all/data_loader.num_batches, end - start))\n",
    "            # print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" .format(\n",
    "            #         b, data_loader.num_batches,\n",
    "            #         e, train_loss, end - start))\n",
    "        np.save('word_embeddings', normalized_embeddings.eval())\n",
    "\n",
    "        final_embeddings = np.load('word_embeddings.npy')\n",
    "        for word in test_words:\n",
    "            if not data_loader.vocab.has_key(word):\n",
    "                continue\n",
    "            word_vec = final_embeddings[data_loader.vocab.get(word),:]\n",
    "            sim_mat = np.matmul(final_embeddings, word_vec)\n",
    "            neareast = (-sim_mat).argsort()[:11]\n",
    "            neareast_words = [data_loader.words[id] for id in neareast]\n",
    "            print 'the nearest words with <{0}> is : {1}'.format(word, ','.join(neareast_words).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the nearest words with <good> is : ju,collateral,rutland,funerals,tangled,auto,stimulate,carolingian,tetragrammaton,schumann\n",
      "the nearest words with <new> is : superfamily,portrayal,silent,lacked,pike,siena,immoral,bec,hezekiah,realtime\n",
      "the nearest words with <one> is : consolation,lowering,rei,survived,foldoc,aluminium,whose,celebration,cranmer,perceptual\n"
     ]
    }
   ],
   "source": [
    "#模型测试    final_embeddings = np.load('word_embeddings.npy')\n",
    "for word in test_words:\n",
    "    if not data_loader.vocab.has_key(word):\n",
    "        continue\n",
    "    word_vec = final_embeddings[data_loader.vocab.get(word),:]\n",
    "    sim_mat = np.matmul(final_embeddings, word_vec)\n",
    "    neareast = (-sim_mat).argsort()[1:11]\n",
    "    neareast_words = [data_loader.words[id] for id in neareast]\n",
    "    print 'the nearest words with <{0}> is : {1}'.format(word, ','.join(neareast_words).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
